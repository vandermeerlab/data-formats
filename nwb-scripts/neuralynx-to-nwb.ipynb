{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An initial attempt at NWB conversion for NeuraLynx data following \"manual\" conversion described in https://pynwb.readthedocs.io/en/stable/tutorials/domain/ecephys.html .  Unlike the example(s) there I (Yarik) was trying to identify levels of data and metadata to consider, and also to store them across multiple .nwb files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pynwb\n",
    "from datetime import datetime\n",
    "from dateutil.tz import tzlocal\n",
    "from pynwb import NWBFile\n",
    "\n",
    "import neo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_data = '../data/BiconditionalOdor/M040-2020-04-28-CDOD11'\n",
    "# session_data = '/Users/jimmiegmaz/Desktop/M040-2020-04-28-CDOD11' #for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common lab wide metadata\n",
    "lab_metadata = dict(\n",
    "    lab=\"MVDMLab\",\n",
    "    institution=\"Dartmouth College\",\n",
    "    keywords=[\"DANDI Pilot\"], # arbitrary, so let's promote!\n",
    ")\n",
    "# Experiment specific one\n",
    "experiment_metadata = dict(\n",
    "    experimenter=\"Jimmie Gmaz <jim.gmaz@gmail.com>\",  # Let's see if nwb can swallow such a record ;)\n",
    "    experiment_description=\"Contextual odor discrimination task\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a reader\n",
    "reader = neo.io.NeuralynxIO(dirname=session_data) # TODO: newer version should support: , keep_original_times=True)\n",
    "reader.parse_header()\n",
    "print(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg = reader.read_segment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "import re\n",
    "filename_metadata = re.match(\n",
    "    '(?P<subject_id>[A-Za-z0-9]*)-(?P<date>20..-..-..)-(?P<task>[A-Za-z]*)(?P<day_of_recording>[0-9]*)$',\n",
    "    op.basename(session_data)).groupdict()\n",
    "assert filename_metadata\n",
    "filename_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Those time stamps are in sub-second and not the one we would want to the \"session time\"\n",
    "# time.gmtime(reader.get_event_timestamps()[0][0])\n",
    "# TODO: figure out where in this \n",
    "# TODO: figure out what those timestamps in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scans through Experimental Keys to extract relevant metadata for NWB file\n",
    "\n",
    "# name of ExpKeys file\n",
    "keys_name = session_data + '/'  + filename_metadata['subject_id'] + '_' + filename_metadata['date'].replace('-','_') + '_keys.m'\n",
    "\n",
    "# read session ExpKeys\n",
    "with open (keys_name, 'rt') as keys_file:\n",
    "    exp_keys = keys_file.read()\n",
    "\n",
    "# list of metadata to extract\n",
    "metadata_list = ['ExpKeys.species','ExpKeys.hemisphere','ExpKeys.weight','ExpKeys.probeDepth','ExpKeys.target']\n",
    "\n",
    "# initialize metadata dictionary\n",
    "metadata_keys = dict.fromkeys(metadata_list)\n",
    "\n",
    "# extract metadata\n",
    "for item in exp_keys.split(\"\\n\"):\n",
    "    for field in metadata_list:\n",
    "        if field in item:\n",
    "            metadata_keys[field] = re.search('(?<=\\=)(.*?)(?=\\;)', item).group(0).strip() \n",
    "            metadata_keys[field] = re.sub('[^A-Za-z0-9]+', '', metadata_keys[field])\n",
    "            print(metadata_keys[field])\n",
    "            \n",
    "# TODO: add surgery details to ExpKeys, including AP and ML coordinates, change probeDepth to mm,\n",
    "# add filtering, individual tetrode depth, tetrode referencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata which is likely to come from data files and \"promotion\" metadata records\n",
    "\n",
    "# Most likely many could be parsed from the filenames which are likely to encode some of it\n",
    "# So \"heuristical\" converter could establish metadata harvesting from the filenames\n",
    "\n",
    "#\n",
    "# Session specific\n",
    "session_metadata = dict(\n",
    "    session_id=\"%(subject_id)s-%(date)s\" % filename_metadata,\n",
    "    session_description=\"Extracellular ephys recording in the left hemisphere of the nucleus accumbens\",  # args[0] in nwbfile\n",
    "    session_start_time=datetime.now(tzlocal()), # TEMP  # args[2] in nwbfile; TODO needs to be datetime\n",
    ")\n",
    "subject_metadata = dict(\n",
    "    subject_id=filename_metadata['subject_id'],\n",
    "    weight=metadata_keys['ExpKeys.weight'],\n",
    "    age=\"TODO\",  # duplicate with session_start_time and date_of_birth but why not?\n",
    "    species=metadata_keys['ExpKeys.species'],\n",
    "    sex=\"female\",\n",
    "#     hemisphere=metadata_keys['ExpKeys.hemisphere'],\n",
    "#     depth=metadata_keys['ExpKeys.probeDepth'],\n",
    "#     region=metadata_keys['ExpKeys.target'],\n",
    "    date_of_birth=datetime.now(tzlocal()), # TEMP: TODO\n",
    ")\n",
    "surgery_metadata = dict(\n",
    "    surgery=\"Headbar on xx/xx/2020, craniotomy over right hemisphere on xx/xx/2020, craniotomy over left hemisphere on xx/xx/2020. All surgeries performed by JG.\"\n",
    ")\n",
    "# Actually probably only \"identifier\" should be file specific, the rest common across files\n",
    "# we would like to produce: separate for .ncs, .ntt, behavioral metadata, etc\n",
    "file_metadata = dict(\n",
    "    source_script=\"somescript-not-clear-whyneeds to be not empty if file_name is provided\",\n",
    "    source_script_file_name=\"TODO\", # __file__,\n",
    ")\n",
    "\n",
    "# common filename prefix - let's mimic DANDI filenaming convention right away\n",
    "filename_prefix = \"sub-{subject_id}_ses-{session_id}\".format(**subject_metadata, **session_metadata)\n",
    "# the rest will be specific to the corresponding file. E.g. we will have separate\n",
    "#  - `_probe-<name>_ecephys.nwb` (from each .ncs) - contineous data from each tetrode. probably chunked and compressed\n",
    "#  - `_???_ecephys.nwb` (from each .ntt) - spike detected windowed data. \n",
    "#  - `_behav.mpg` + `_behav.nwb` - video recording and metadata (including those .png?) for behavior component within experiment recording session\n",
    "# Pretty much we need to establish a framework where EVERY file present would be\n",
    "# provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below would need to follow common pattern \n",
    "- create a new NWBFile with common metadata,\n",
    "- populate with relevant data and metadata\n",
    "- save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Such NWBFile will be created for each separate file, and then fill up with the corresponding\n",
    "#\n",
    "filename_suffix = \"TODO\"\n",
    "nwbfile = NWBFile(\n",
    "    identifier=\"{}_{}\".format(filename_prefix, filename_suffix), # args[1] in nwbfile, may be just UUID? not sure why user has to provide it really\n",
    "    subject=pynwb.file.Subject(**subject_metadata),\n",
    "    **lab_metadata,\n",
    "    **experiment_metadata,\n",
    "    **session_metadata,\n",
    "    **surgery_metadata,\n",
    "    **file_metadata,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nwbfile.identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add electrode metadata\n",
    "# create probe device\n",
    "device = nwbfile.create_device(name='silicon probe', description='A4x2-tet-5mm-150-200-121', manufacturer='NeuroNexus')\n",
    "\n",
    "# for each channel on the probe\n",
    "for chl in reader.header['unit_channels']:\n",
    "    \n",
    "    # get tetrode id\n",
    "    tetrode = re.search('(?<=TT)(.*?)(?=#)', chl[0]).group(0)\n",
    "    electrode_name = 'tetrode' + tetrode\n",
    "    \n",
    "    # get channel id\n",
    "    channel = re.search('(?<=#)(.*?)(?=#)', chl[0]).group(0)\n",
    "           \n",
    "    if electrode_name not in nwbfile.electrode_groups: # make tetrode if does not exist\n",
    "    \n",
    "        description = electrode_name\n",
    "        location = metadata_keys['ExpKeys.hemisphere'] + ' ' + metadata_keys['ExpKeys.target'] + ' ' + \\\n",
    "            '(' + metadata_keys['ExpKeys.probeDepth'] + ' um)'\n",
    "\n",
    "        electrode_group = nwbfile.create_electrode_group(electrode_name,\n",
    "                                                         description=description,\n",
    "                                                         location=location,\n",
    "                                                         device=device)\n",
    "        \n",
    "    # add channel to tetrode\n",
    "    nwbfile.add_electrode(id=int(channel),\n",
    "                          x=-1.2, y=float(metadata_keys['ExpKeys.probeDepth']), z=-1.5,\n",
    "                          location=metadata_keys['ExpKeys.target'], filtering='none',\n",
    "                          imp = 0.0, group=nwbfile.electrode_groups[electrode_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add data\n",
    "from pynwb.ecephys import ElectricalSeries\n",
    "from pynwb.ecephys import SpikeEventSeries\n",
    "from pynwb.ecephys import EventWaveform\n",
    "\n",
    "# add .ntt files\n",
    "ephys_waveform = EventWaveform()\n",
    "\n",
    "# loop through .ntt files\n",
    "for i, chl in enumerate(reader.header['unit_channels']):\n",
    "    \n",
    "    # get tetrode id\n",
    "    tetrode = re.search('(?<=TT)(.*?)(?=#)', chl[0]).group(0)\n",
    "    tetrode_name = 'TT' + tetrode\n",
    "           \n",
    "    if tetrode_name not in ephys_waveform.spike_event_series: # make tetrode if does not exist\n",
    "        \n",
    "        chl_list = []\n",
    "        \n",
    "        for j, group in enumerate(nwbfile.electrodes['group']):\n",
    "        \n",
    "            if tetrode in nwbfile.electrodes['group'][j].fields['description']:\n",
    "                \n",
    "                chl_list.append(j)\n",
    "        \n",
    "        electrode_table_region = nwbfile.create_electrode_table_region(chl_list, tetrode_name)\n",
    "    \n",
    "        waveform = reader.get_spike_raw_waveforms(unit_index=i)\n",
    "\n",
    "        ephys_waveform.create_spike_event_series(tetrode_name,\n",
    "                                                 waveform,\n",
    "                                                 seg.spiketrains[i].times,\n",
    "                                                 electrode_table_region)\n",
    "\n",
    "nwbfile.add_acquisition(ephys_waveform)\n",
    "\n",
    "# add .ncs files\n",
    "\n",
    "chl_list = []\n",
    "\n",
    "for chl in reader.header['signal_channels']['id']:\n",
    "    \n",
    "    chl_list.append(nwbfile.electrodes['id'][:].index(chl))\n",
    "    \n",
    "electrode_table_region = nwbfile.create_electrode_table_region(chl_list, 'CSC order for time series')\n",
    "\n",
    "ephys_ts = ElectricalSeries('CSC data',\n",
    "                            seg.analogsignals[0].magnitude,\n",
    "                            electrode_table_region,\n",
    "                            timestamps=seg.analogsignals[0].times,\n",
    "                            comments='n/a',\n",
    "                            description='unfiltered CSC data')\n",
    "\n",
    "nwbfile.add_acquisition(ephys_ts)\n",
    "\n",
    "# nwbfile.add_unit(id=1, electrodes=[0])\n",
    "# nwbfile.add_unit(id=2, electrodes=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the generated file\n",
    "from pynwb import NWBHDF5IO\n",
    "\n",
    "# TODO: I think we should right away use dandi-cli provided API to create the filename based on metadata\n",
    "# in the NWBFile\n",
    "with NWBHDF5IO('BCD_example.nwb', 'w') as io:\n",
    "    io.write(nwbfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
