{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An initial attempt at NWB conversion for NeuraLynx data following \"manual\" conversion described in https://pynwb.readthedocs.io/en/stable/tutorials/domain/ecephys.html .  Unlike the example(s) there I (Yarik) was trying to identify levels of data and metadata to consider, and also to store them across multiple .nwb files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pynwb\n",
    "from datetime import datetime\n",
    "from dateutil.tz import tzlocal\n",
    "from pynwb import NWBFile\n",
    "\n",
    "import neo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_data = '../data/BiconditionalOdor/M040-2020-04-28-CDOD11'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common lab wide metadata\n",
    "lab_metadata = dict(\n",
    "    lab=\"MVDMLab\",\n",
    "    institution=\"Dartmouth College\",\n",
    "    keywords=[\"DANDI Pilot\"], # arbitrary, so let's promote!\n",
    ")\n",
    "# Experiment specific one\n",
    "experiment_metadata = dict(\n",
    "    experimenter=\"Jimmie Gmaz <jim.gmaz@gmail.com>\",  # Let's see if nwb can swallow such a record ;)\n",
    "    experiment_description=\"Whatever Jimmi did and why\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralynxIO: ../data/BiconditionalOdor/M040-2020-04-28-CDOD11/\n",
      "nb_block: 1\n",
      "nb_segment:  [7]\n",
      "signal_channels: [CSC1, CSC3, CSC8]\n",
      "unit_channels: [chTT1#26#0, chTT1#10#0, chTT1#29#0, chTT1#30#0]\n",
      "event_channels: [Events event_id=11 ttl=0, Events event_id=11 ttl=1, Events event_id=11 ttl=2, Events event_id=11 ttl=4 ... Events event_id=11 ttl=48 Events event_id=11 ttl=64 Events event_id=11 ttl=96 Events event_id=19 ttl=0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a reader\n",
    "reader = neo.io.NeuralynxIO(dirname=session_data) # TODO: newer version should support: , keep_original_times=True)\n",
    "reader.parse_header()\n",
    "print(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subject_id': 'M040',\n",
       " 'date': '2020-04-28',\n",
       " 'task': 'CDOD',\n",
       " 'day_of_recording': '11'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os.path as op\n",
    "import re\n",
    "filename_metadata = re.match(\n",
    "    '(?P<subject_id>[A-Za-z0-9]*)-(?P<date>20..-..-..)-(?P<task>[A-Za-z]*)(?P<day_of_recording>[0-9]*)$',\n",
    "    op.basename(session_data)).groupdict()\n",
    "assert filename_metadata\n",
    "filename_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Those time stamps are in sub-second and not the one we would want to the \"session time\"\n",
    "# time.gmtime(reader.get_event_timestamps()[0][0])\n",
    "# TODO: figure out where in this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: figure out what those timestamps in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata which is likely to come from data files and \"promotion\" metadata records\n",
    "\n",
    "# Most likely many could be parsed from the filenames which are likely to encode some of it\n",
    "# So \"heuristical\" converter could establish metadata harvesting from the filenames\n",
    "\n",
    "#\n",
    "# Session specific\n",
    "session_metadata = dict(\n",
    "    session_id=\"%(subject_id)s-%(date)s\" % filename_metadata,\n",
    "    session_description=\"Extracellular ephys recording in .... TODO ... \",  # args[0] in nwbfile\n",
    "    session_start_time=datetime.now(tzlocal()), # TEMP  # args[2] in nwbfile; TODO needs to be datetime\n",
    ")\n",
    "subject_metadata = dict(\n",
    "    subject_id=\"TODO\",\n",
    "    weight=\"TODO\",\n",
    "    age=\"TODO\",  # duplicate with session_start_time and date_of_birth but why not?\n",
    "    species=\"Mus musculus\",\n",
    "    sex=\"TODO\",\n",
    "    date_of_birth=datetime.now(tzlocal()), # TEMP: TODO\n",
    ")\n",
    "surgery_metadata = dict(\n",
    "    surgery=\"TODO: Narrative description about surgery/surgeries, including date(s) and who performed surgery.\"\n",
    ")\n",
    "# Actually probably only \"identifier\" should be file specific, the rest common across files\n",
    "# we would like to produce: separate for .ncs, .ntt, behavioral metadata, etc\n",
    "file_metadata = dict(\n",
    "    source_script=\"somescript-not-clear-whyneeds to be not empty if file_name is provided\",\n",
    "    source_script_file_name=\"TODO\", # __file__,\n",
    ")\n",
    "\n",
    "# common filename prefix - let's mimic DANDI filenaming convention right away\n",
    "filename_prefix = \"sub-{subject_id}_ses-{session_id}\".format(**subject_metadata, **session_metadata)\n",
    "# the rest will be specific to the corresponding file. E.g. we will have separate\n",
    "#  - `_probe-<name>_ecephys.nwb` (from each .ncs) - contineous data from each tetrode. probably chunked and compressed\n",
    "#  - `_???_ecephys.nwb` (from each .ntt) - spike detected windowed data. \n",
    "#  - `_behav.mpg` + `_behav.nwb` - video recording and metadata (including those .png?) for behavior component within experiment recording session\n",
    "# Pretty much we need to establish a framework where EVERY file present would be\n",
    "# provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subject_id': 'TODO',\n",
       " 'weight': 'TODO',\n",
       " 'age': 'TODO',\n",
       " 'species': 'Mus musculus',\n",
       " 'sex': 'TODO',\n",
       " 'date_of_birth': datetime.datetime(2020, 6, 22, 16, 39, 41, 154839, tzinfo=tzlocal())}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below would need to follow common pattern \n",
    "- create a new NWBFile with common metadata,\n",
    "- populate with relevant data and metadata\n",
    "- save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Such NWBFile will be created for each separate file, and then fill up with the corresponding\n",
    "#\n",
    "filename_suffix = \"TODO\"\n",
    "nwbfile = NWBFile(\n",
    "    identifier=\"{}_{}\".format(filename_prefix, filename_suffix), # args[1] in nwbfile, may be just UUID? not sure why user has to provide it really\n",
    "    subject=pynwb.file.Subject(**subject_metadata),\n",
    "    **lab_metadata,\n",
    "    **experiment_metadata,\n",
    "    **session_metadata,\n",
    "    **surgery_metadata,\n",
    "    **file_metadata,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-TODO_ses-TODO_TODO\n"
     ]
    }
   ],
   "source": [
    "print(nwbfile.identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy/pasted example - yet to be tuned for data at hands.\n",
    "# Most likely we would want to establish Neo-based \"sweep and convert\".\n",
    "# but somehow need to ensure that we had not forgotten any piece of data/file.\n",
    "device = nwbfile.create_device(name='trodes_rig123')\n",
    "\n",
    "electrode_name = 'tetrode1'\n",
    "description = \"an example tetrode\"\n",
    "location = \"somewhere in the hippocampus\"\n",
    "\n",
    "electrode_group = nwbfile.create_electrode_group(electrode_name,\n",
    "                                                 description=description,\n",
    "                                                 location=location,\n",
    "                                                 device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Save the generated file\n",
    "from pynwb import NWBHDF5IO\n",
    "\n",
    "# TODO: I think we should right away use dandi-cli provided API to create the filename based on metadata\n",
    "# in the NWBFile\n",
    "with NWBHDF5IO('ecephys_example.nwb', 'w') as io:\n",
    "    io.write(nwbfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (default available)",
   "language": "python",
   "name": "default-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
