{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An initial attempt at NWB conversion for NeuraLynx data following \"manual\" conversion described in https://pynwb.readthedocs.io/en/stable/tutorials/domain/ecephys.html .  Unlike the example(s) there I (Yarik) was trying to identify levels of data and metadata to consider, and also to store them across multiple .nwb files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pynwb\n",
    "from datetime import datetime\n",
    "from dateutil.tz import tzlocal\n",
    "from pynwb import NWBFile\n",
    "\n",
    "import neo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#session_data = '../data/BiconditionalOdor/M040-2020-04-28-CDOD11'\n",
    "session_data = '/Users/jimmiegmaz/Desktop/M040-2020-04-28-CDOD11' #for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common lab wide metadata\n",
    "lab_metadata = dict(\n",
    "    lab=\"MVDMLab\",\n",
    "    institution=\"Dartmouth College\",\n",
    "    keywords=[\"DANDI Pilot\"], # arbitrary, so let's promote!\n",
    ")\n",
    "# Experiment specific one\n",
    "experiment_metadata = dict(\n",
    "    experimenter=\"Jimmie Gmaz <jim.gmaz@gmail.com>\",  # Let's see if nwb can swallow such a record ;)\n",
    "    experiment_description=\"Contextual odor discrimination task\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralynxIO: /Users/jimmiegmaz/Desktop/M040-2020-04-28-CDOD11\n",
      "nb_block: 1\n",
      "nb_segment:  [7]\n",
      "signal_channels: [CSC10, CSC13, CSC15, CSC2, CSC4, CSC5]\n",
      "unit_channels: [chTT2#27#0, chTT2#28#0, chTT2#11#0, chTT2#12#0, chTT4#31#0, chTT4#15#0, chTT4#9#0, chTT4#25#0]\n",
      "event_channels: [Events event_id=11 ttl=0, Events event_id=11 ttl=1, Events event_id=11 ttl=2, Events event_id=11 ttl=4 ... Events event_id=11 ttl=48 Events event_id=11 ttl=64 Events event_id=11 ttl=96 Events event_id=19 ttl=0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a reader\n",
    "reader = neo.io.NeuralynxIO(dirname=session_data) # TODO: newer version should support: , keep_original_times=True)\n",
    "reader.parse_header()\n",
    "print(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nb_block': 1, 'nb_segment': [7], 'signal_channels': array([('CSC10', 31, 30000., 'int16', 'uV', -0.06103516, 0., 0),\n",
      "       ('CSC13',  9, 30000., 'int16', 'uV', -0.12207031, 0., 0),\n",
      "       ('CSC15', 25, 30000., 'int16', 'uV', -0.06103516, 0., 0),\n",
      "       ('CSC2', 27, 30000., 'int16', 'uV', -0.06103516, 0., 0),\n",
      "       ('CSC4', 28, 30000., 'int16', 'uV', -0.06103516, 0., 0),\n",
      "       ('CSC5', 11, 30000., 'int16', 'uV', -0.12207031, 0., 0)],\n",
      "      dtype=[('name', '<U64'), ('id', '<i8'), ('sampling_rate', '<f8'), ('dtype', '<U16'), ('units', '<U64'), ('gain', '<f8'), ('offset', '<f8'), ('group_id', '<i8')]), 'unit_channels': array([('chTT2#27#0', '0', 'uV', -0.00762939, 0., -1, 30000.),\n",
      "       ('chTT2#28#0', '0', 'uV', -0.00762939, 0., -1, 30000.),\n",
      "       ('chTT2#11#0', '0', 'uV', -0.00762939, 0., -1, 30000.),\n",
      "       ('chTT2#12#0', '0', 'uV', -0.00762939, 0., -1, 30000.),\n",
      "       ('chTT4#31#0', '0', 'uV', -0.00610352, 0., -1, 30000.),\n",
      "       ('chTT4#15#0', '0', 'uV', -0.00610352, 0., -1, 30000.),\n",
      "       ('chTT4#9#0', '0', 'uV', -0.00610352, 0., -1, 30000.),\n",
      "       ('chTT4#25#0', '0', 'uV', -0.00610352, 0., -1, 30000.)],\n",
      "      dtype=[('name', '<U64'), ('id', '<U64'), ('wf_units', '<U64'), ('wf_gain', '<f8'), ('wf_offset', '<f8'), ('wf_left_sweep', '<i8'), ('wf_sampling_rate', '<f8')]), 'event_channels': array([('Events event_id=11 ttl=0', 'Events', b'event'),\n",
      "       ('Events event_id=11 ttl=1', 'Events', b'event'),\n",
      "       ('Events event_id=11 ttl=2', 'Events', b'event'),\n",
      "       ('Events event_id=11 ttl=4', 'Events', b'event'),\n",
      "       ('Events event_id=11 ttl=8', 'Events', b'event'),\n",
      "       ('Events event_id=11 ttl=16', 'Events', b'event'),\n",
      "       ('Events event_id=11 ttl=40', 'Events', b'event'),\n",
      "       ('Events event_id=11 ttl=48', 'Events', b'event'),\n",
      "       ('Events event_id=11 ttl=64', 'Events', b'event'),\n",
      "       ('Events event_id=11 ttl=96', 'Events', b'event'),\n",
      "       ('Events event_id=19 ttl=0', 'Events', b'event')],\n",
      "      dtype=[('name', '<U64'), ('id', '<U64'), ('type', 'S5')])}\n"
     ]
    }
   ],
   "source": [
    "print(reader.header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = reader.get_spike_raw_waveforms(unit_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a[6000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_unit = reader.unit_channels_count()\n",
    "print('nb_unit', nb_unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = reader.header['unit_channels'][0]['name']\n",
    "print(units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader.header.dtype()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg = reader.read_segment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(seg.spiketrains[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(seg.spiketrains[0].times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date': '2020-04-28',\n",
       " 'day_of_recording': '11',\n",
       " 'subject_id': 'M040',\n",
       " 'task': 'CDOD'}"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os.path as op\n",
    "import re\n",
    "filename_metadata = re.match(\n",
    "    '(?P<subject_id>[A-Za-z0-9]*)-(?P<date>20..-..-..)-(?P<task>[A-Za-z]*)(?P<day_of_recording>[0-9]*)$',\n",
    "    op.basename(session_data)).groupdict()\n",
    "assert filename_metadata\n",
    "filename_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Those time stamps are in sub-second and not the one we would want to the \"session time\"\n",
    "# time.gmtime(reader.get_event_timestamps()[0][0])\n",
    "# TODO: figure out where in this \n",
    "# TODO: figure out what those timestamps in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mouse\n",
      "left\n",
      "vStr\n",
      "4200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scans through Experimental Keys to extract relevant metadata for NWB file\n",
    "\n",
    "# name of ExpKeys file\n",
    "keys_name = session_data + '/'  + filename_metadata['subject_id'] + '_' + filename_metadata['date'].replace('-','_') + '_keys.m'\n",
    "\n",
    "# read session ExpKeys\n",
    "with open (keys_name, 'rt') as keys_file:\n",
    "    exp_keys = keys_file.read()\n",
    "\n",
    "# list of metadata to extract\n",
    "metadata_list = ['ExpKeys.species','ExpKeys.hemisphere','ExpKeys.weight','ExpKeys.probeDepth','ExpKeys.target']\n",
    "\n",
    "# initialize metadata dictionary\n",
    "metadata_keys = dict.fromkeys(metadata_list)\n",
    "\n",
    "# extract metadata\n",
    "for item in exp_keys.split(\"\\n\"):\n",
    "    for field in metadata_list:\n",
    "        if field in item:\n",
    "            metadata_keys[field] = re.search('(?<=\\=)(.*?)(?=\\;)', item).group(0).strip() \n",
    "            metadata_keys[field] = re.sub('[^A-Za-z0-9]+', '', metadata_keys[field])\n",
    "            print(metadata_keys[field])\n",
    "            \n",
    "# TODO: add surgery details to ExpKeys, including AP and ML coordinates, change probeDepth to mm,\n",
    "# add filtering, individual tetrode depth, tetrode referencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata which is likely to come from data files and \"promotion\" metadata records\n",
    "\n",
    "# Most likely many could be parsed from the filenames which are likely to encode some of it\n",
    "# So \"heuristical\" converter could establish metadata harvesting from the filenames\n",
    "\n",
    "#\n",
    "# Session specific\n",
    "session_metadata = dict(\n",
    "    session_id=\"%(subject_id)s-%(date)s\" % filename_metadata,\n",
    "    session_description=\"Extracellular ephys recording in the left hemisphere of the nucleus accumbens\",  # args[0] in nwbfile\n",
    "    session_start_time=datetime.now(tzlocal()), # TEMP  # args[2] in nwbfile; TODO needs to be datetime\n",
    ")\n",
    "subject_metadata = dict(\n",
    "    subject_id=filename_metadata['subject_id'],\n",
    "    weight=metadata_keys['ExpKeys.weight'],\n",
    "    age=\"TODO\",  # duplicate with session_start_time and date_of_birth but why not?\n",
    "    species=metadata_keys['ExpKeys.species'],\n",
    "    sex=\"female\",\n",
    "#     hemisphere=metadata_keys['ExpKeys.hemisphere'],\n",
    "#     depth=metadata_keys['ExpKeys.probeDepth'],\n",
    "#     region=metadata_keys['ExpKeys.target'],\n",
    "    date_of_birth=datetime.now(tzlocal()), # TEMP: TODO\n",
    ")\n",
    "surgery_metadata = dict(\n",
    "    surgery=\"Headbar on xx/xx/2020, craniotomy over right hemisphere on xx/xx/2020, craniotomy over left hemisphere on xx/xx/2020. All surgeries performed by JG.\"\n",
    ")\n",
    "# Actually probably only \"identifier\" should be file specific, the rest common across files\n",
    "# we would like to produce: separate for .ncs, .ntt, behavioral metadata, etc\n",
    "file_metadata = dict(\n",
    "    source_script=\"somescript-not-clear-whyneeds to be not empty if file_name is provided\",\n",
    "    source_script_file_name=\"TODO\", # __file__,\n",
    ")\n",
    "\n",
    "# common filename prefix - let's mimic DANDI filenaming convention right away\n",
    "filename_prefix = \"sub-{subject_id}_ses-{session_id}\".format(**subject_metadata, **session_metadata)\n",
    "# the rest will be specific to the corresponding file. E.g. we will have separate\n",
    "#  - `_probe-<name>_ecephys.nwb` (from each .ncs) - contineous data from each tetrode. probably chunked and compressed\n",
    "#  - `_???_ecephys.nwb` (from each .ntt) - spike detected windowed data. \n",
    "#  - `_behav.mpg` + `_behav.nwb` - video recording and metadata (including those .png?) for behavior component within experiment recording session\n",
    "# Pretty much we need to establish a framework where EVERY file present would be\n",
    "# provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': 'TODO',\n",
       " 'date_of_birth': datetime.datetime(2020, 12, 29, 14, 37, 15, 438748, tzinfo=tzlocal()),\n",
       " 'sex': 'female',\n",
       " 'species': 'Mouse',\n",
       " 'subject_id': 'M040',\n",
       " 'weight': ''}"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below would need to follow common pattern \n",
    "- create a new NWBFile with common metadata,\n",
    "- populate with relevant data and metadata\n",
    "- save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Such NWBFile will be created for each separate file, and then fill up with the corresponding\n",
    "#\n",
    "filename_suffix = \"TODO\"\n",
    "nwbfile = NWBFile(\n",
    "    identifier=\"{}_{}\".format(filename_prefix, filename_suffix), # args[1] in nwbfile, may be just UUID? not sure why user has to provide it really\n",
    "    subject=pynwb.file.Subject(**subject_metadata),\n",
    "    **lab_metadata,\n",
    "    **experiment_metadata,\n",
    "    **session_metadata,\n",
    "    **surgery_metadata,\n",
    "    **file_metadata,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-M040_ses-M040-2020-04-28_TODO\n"
     ]
    }
   ],
   "source": [
    "print(nwbfile.identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add electrode metadata\n",
    "# create probe device\n",
    "device = nwbfile.create_device(name='silicon probe', description='A4x2-tet-5mm-150-200-121', manufacturer='NeuroNexus')\n",
    "\n",
    "# for each channel on the probe\n",
    "for chl in reader.header['unit_channels']:\n",
    "    \n",
    "    # get tetrode id\n",
    "    tetrode = re.search('(?<=TT)(.*?)(?=#)', chl[0]).group(0)\n",
    "    electrode_name = 'tetrode' + tetrode\n",
    "    \n",
    "    # get channel id\n",
    "    channel = re.search('(?<=#)(.*?)(?=#)', chl[0]).group(0)\n",
    "           \n",
    "    if electrode_name not in nwbfile.electrode_groups: # make tetrode if does not exist\n",
    "    \n",
    "        description = electrode_name\n",
    "        location = metadata_keys['ExpKeys.hemisphere'] + ' ' + metadata_keys['ExpKeys.target'] + ' ' + \\\n",
    "            '(' + metadata_keys['ExpKeys.probeDepth'] + ' um)'\n",
    "\n",
    "        electrode_group = nwbfile.create_electrode_group(electrode_name,\n",
    "                                                         description=description,\n",
    "                                                         location=location,\n",
    "                                                         device=device)\n",
    "        \n",
    "    # add channel to tetrode\n",
    "    nwbfile.add_electrode(id=int(channel),\n",
    "                          x=-1.2, y=float(metadata_keys['ExpKeys.probeDepth']), z=-1.5,\n",
    "                          location=metadata_keys['ExpKeys.target'], filtering='none',\n",
    "                          imp = 0.0, group=nwbfile.electrode_groups[electrode_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add data\n",
    "from pynwb.ecephys import ElectricalSeries\n",
    "from pynwb.ecephys import SpikeEventSeries\n",
    "from pynwb.ecephys import EventWaveform\n",
    "\n",
    "# add .ntt files\n",
    "ephys_waveform = EventWaveform()\n",
    "\n",
    "# loop through .ntt files\n",
    "for i, chl in enumerate(reader.header['unit_channels']):\n",
    "    \n",
    "    # get tetrode id\n",
    "    tetrode = re.search('(?<=TT)(.*?)(?=#)', chl[0]).group(0)\n",
    "    tetrode_name = 'TT' + tetrode\n",
    "           \n",
    "    if tetrode_name not in ephys_waveform.spike_event_series: # make tetrode if does not exist\n",
    "        \n",
    "        chl_list = []\n",
    "        \n",
    "        for j, group in enumerate(nwbfile.electrodes['group']):\n",
    "        \n",
    "            if tetrode in nwbfile.electrodes['group'][j].fields['description']:\n",
    "                \n",
    "                chl_list.append(j)\n",
    "        \n",
    "        electrode_table_region = nwbfile.create_electrode_table_region(chl_list, tetrode_name)\n",
    "    \n",
    "        waveform = reader.get_spike_raw_waveforms(unit_index=i)\n",
    "\n",
    "        ephys_waveform.create_spike_event_series(tetrode_name,\n",
    "                                                 waveform,\n",
    "                                                 seg.spiketrains[i].times,\n",
    "                                                 electrode_table_region)\n",
    "\n",
    "# nwbfile.add_acquisition(ephys_waveform)\n",
    "\n",
    "# add .ncs files\n",
    "\n",
    "chl_list = []\n",
    "\n",
    "for chl in reader.header['signal_channels']['id']:\n",
    "    \n",
    "    chl_list.append(nwbfile.electrodes['id'][:].index(chl))\n",
    "    \n",
    "electrode_table_region = nwbfile.create_electrode_table_region(chl_list, 'CSC order for time series')\n",
    "\n",
    "ephys_ts = ElectricalSeries('CSC data',\n",
    "                            seg.analogsignals[0].magnitude,\n",
    "                            electrode_table_region,\n",
    "                            timestamps=seg.analogsignals[0].times,\n",
    "                            comments='n/a',\n",
    "                            description='unfiltered CSC data')\n",
    "# nwbfile.add_acquisition(ephys_ts)\n",
    "\n",
    "# nwbfile.add_unit(id=1, electrodes=[0])\n",
    "# nwbfile.add_unit(id=2, electrodes=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the generated file\n",
    "from pynwb import NWBHDF5IO\n",
    "\n",
    "# TODO: I think we should right away use dandi-cli provided API to create the filename based on metadata\n",
    "# in the NWBFile\n",
    "with NWBHDF5IO('BCD_example.nwb', 'w') as io:\n",
    "    io.write(nwbfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
